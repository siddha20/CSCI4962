{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "\n",
    "The deep learning framework I will be using is PyTorch. PyTorch and deep\n",
    "learning frameworks generally utilize GPUs to speed up the computation of\n",
    "matrix operations (neural networks are essentially a collection of matrices).\n",
    "Additionally, these frameworks have front-ends allowing users to declare what\n",
    "kind of feed-forward network model they want. Moreover, they also contain a\n",
    "back-end to compute the gradient according to some cost function, which the\n",
    "user can also pick. \n",
    "\n",
    "The only resource I need is https://pytorch.org/docs/stable/index.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "\n",
    "The dataset I will be using comes from here:\n",
    "https://www.kaggle.com/datasets/gauravtopre/bank-customer-churn-dataset. The\n",
    "goal of this data set is to predict the customer churn for a bank. The dataset\n",
    "has 10000 data entries, which should be enough for a deep learning model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>credit_score</th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>tenure</th>\n",
       "      <th>balance</th>\n",
       "      <th>products_number</th>\n",
       "      <th>credit_card</th>\n",
       "      <th>active_member</th>\n",
       "      <th>estimated_salary</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15634602</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15647311</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15619304</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15701354</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15737888</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>15606229</td>\n",
       "      <td>771</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96270.64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>15569892</td>\n",
       "      <td>516</td>\n",
       "      <td>France</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>57369.61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101699.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>15584532</td>\n",
       "      <td>709</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>42085.58</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>15682355</td>\n",
       "      <td>772</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Male</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>75075.31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>92888.52</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>15628319</td>\n",
       "      <td>792</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>130142.79</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38190.78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      customer_id  credit_score  country  gender  age  tenure    balance  \\\n",
       "0        15634602           619   France  Female   42       2       0.00   \n",
       "1        15647311           608    Spain  Female   41       1   83807.86   \n",
       "2        15619304           502   France  Female   42       8  159660.80   \n",
       "3        15701354           699   France  Female   39       1       0.00   \n",
       "4        15737888           850    Spain  Female   43       2  125510.82   \n",
       "...           ...           ...      ...     ...  ...     ...        ...   \n",
       "9995     15606229           771   France    Male   39       5       0.00   \n",
       "9996     15569892           516   France    Male   35      10   57369.61   \n",
       "9997     15584532           709   France  Female   36       7       0.00   \n",
       "9998     15682355           772  Germany    Male   42       3   75075.31   \n",
       "9999     15628319           792   France  Female   28       4  130142.79   \n",
       "\n",
       "      products_number  credit_card  active_member  estimated_salary  churn  \n",
       "0                   1            1              1         101348.88      1  \n",
       "1                   1            0              1         112542.58      0  \n",
       "2                   3            1              0         113931.57      1  \n",
       "3                   2            0              0          93826.63      0  \n",
       "4                   1            1              1          79084.10      0  \n",
       "...               ...          ...            ...               ...    ...  \n",
       "9995                2            1              0          96270.64      0  \n",
       "9996                1            1              1         101699.77      0  \n",
       "9997                1            0              1          42085.58      1  \n",
       "9998                2            1              0          92888.52      1  \n",
       "9999                1            1              0          38190.78      0  \n",
       "\n",
       "[10000 rows x 12 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "df = pd.read_csv(\"bank_churn_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table above, we can see that the data set includes factors such\n",
    "as credit score, country, gender, age, balance, and salary. These factors will\n",
    "be used in the deep learning model. Some factors are not used on purpose so\n",
    "that the input parameters to the model can be more general. However, this might\n",
    "be changed later on after evaluating the results. \n",
    "\n",
    "Looking below at the statistical breakdown of the data, we can see that the\n",
    "dataset is nice and balanced: the male-to-female ratio is almost one-half and\n",
    "there is a wide range for salary, credit score, and age. The only thing lacking\n",
    "is the countries because there are only 3: France, Germany, and Spain. However,\n",
    "if this model is used only for banks in those countries, then it would not be\n",
    "an issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 6) (10000, 1)\n",
      "age mean: 38.9218, std: 10.487282048271611\n",
      "credit_score mean: 650.5288, std: 96.64846595037089\n",
      "salary mean: 100090.239881, std: 57507.617221165565\n",
      "percent male: 0.5457\n",
      "countries: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def norm(x):\n",
    "    return (x - np.min(x))/(np.max(x) - np.min(x))\n",
    "\n",
    "gender = norm(np.asarray([df[\"gender\"] == \"Male\"], dtype=np.float64))\n",
    "credit_score = norm(np.asarray(df[\"credit_score\"], dtype=np.float64))\n",
    "age = norm(np.asarray(df[\"age\"], dtype=np.float64))\n",
    "balance = norm(np.asarray(df[\"balance\"], dtype=np.float64))\n",
    "credit_score = norm(np.asarray(df[\"credit_score\"], dtype=np.float64))\n",
    "salary = norm(np.asarray(df[\"estimated_salary\"], dtype=np.float64))\n",
    "\n",
    "country_dict = {}\n",
    "id = 0\n",
    "for c in df[\"country\"]:\n",
    "    if not c in country_dict.keys():\n",
    "        id += 1\n",
    "        country_dict[c] = id\n",
    "countries = norm(np.asarray([country_dict[c] for c in df[\"country\"]], dtype=np.float64))\n",
    "\n",
    "bank_data = np.vstack((credit_score, countries, gender, age, balance, salary)).T\n",
    "bank_churn = np.asarray([df[\"churn\"]], dtype=np.float64).T\n",
    "print(bank_data.shape, bank_churn.shape)\n",
    "\n",
    "print(\"age mean: {}, std: {}\".format(np.mean(df[\"age\"]), np.std(df[\"age\"])))\n",
    "print(\"credit_score mean: {}, std: {}\".format(np.mean(df[\"credit_score\"]), np.std(df[\"credit_score\"])))\n",
    "print(\"salary mean: {}, std: {}\".format(np.mean(df[\"estimated_salary\"]), np.std(df[\"estimated_salary\"])))\n",
    "print(\"percent male: {}\".format(np.mean(gender)))\n",
    "print(\"countries: {}\".format(len(country_dict)))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title(\"Box Plot for Age\")\n",
    "plt.boxplot(df[\"age\"])\n",
    "plt.figure(2)\n",
    "plt.title(\"Box Plot for Salary\")\n",
    "plt.boxplot(df[\"estimated_salary\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class BasicLoader(Dataset):\n",
    "   def __init__(self, x, y):\n",
    "      self.X = torch.Tensor(x)\n",
    "      self.Y = torch.Tensor(y)\n",
    "      assert(self.X.shape[0] == self.Y.shape[0])\n",
    "\n",
    "   def __len__(self):\n",
    "      return self.X.shape[0]\n",
    "      \n",
    "   def __getitem__(self, index):\n",
    "      return self.X[index], self.Y[index]\n",
    "\n",
    "\n",
    "# split data into 50% train 25% val 25% test\n",
    "train_bank_data, test_bank_data, train_bank_churn, test_bank_churn = \\\n",
    "   train_test_split(bank_data, bank_churn, test_size=.25, random_state=30)\n",
    "train_bank_data, val_bank_data, train_bank_churn, val_bank_churn = \\\n",
    "   train_test_split(train_bank_data, train_bank_churn, test_size=.3333, random_state=30)\n",
    "\n",
    "train_data_set = BasicLoader(train_bank_data, train_bank_churn)\n",
    "val_data_set = BasicLoader(val_bank_data, val_bank_churn)\n",
    "test_data_set = BasicLoader(test_bank_data, test_bank_churn)\n",
    "\n",
    "train_data_loader = DataLoader(train_data_set, batch_size=64, shuffle=True)\n",
    "val_data_loader = DataLoader(val_data_set, batch_size=64, shuffle=True)\n",
    "test_data_loader = DataLoader(test_data_set, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn \n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, hl1, hl2):\n",
    "        super(NN, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(6, hl1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hl1, hl2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hl2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "    \n",
    "model = NN(200, 200).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "epochs = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.6801129579544067\n",
      "train loss 0.42949071526527405\n",
      "train loss 0.4346965253353119\n",
      "train loss 0.4554115831851959\n",
      "train loss 0.4278102219104767\n",
      "train loss 0.2763051390647888\n",
      "train loss 0.38337838649749756\n",
      "train loss 0.4042637348175049\n",
      "train loss 0.32447782158851624\n",
      "train loss 0.4024965465068817\n",
      "train loss 0.3089750111103058\n",
      "train loss 0.4004029631614685\n",
      "train loss 0.4926159977912903\n",
      "train loss 0.4062790870666504\n",
      "train loss 0.38922905921936035\n",
      "train loss 0.43216636776924133\n",
      "train loss 0.28247013688087463\n",
      "train loss 0.515982985496521\n",
      "train loss 0.3299247622489929\n",
      "train loss 0.5104387998580933\n",
      "train loss 0.45200157165527344\n",
      "train loss 0.4187867343425751\n",
      "train loss 0.42384639382362366\n",
      "train loss 0.3558008074760437\n",
      "train loss 0.4136687219142914\n",
      "train loss 0.3794749081134796\n",
      "train loss 0.4768986999988556\n",
      "train loss 0.4420997202396393\n",
      "train loss 0.32830706238746643\n",
      "train loss 0.35461437702178955\n",
      "train loss 0.39615681767463684\n",
      "train loss 0.42920777201652527\n",
      "train loss 0.45688003301620483\n",
      "train loss 0.46855050325393677\n",
      "train loss 0.4721275568008423\n",
      "train loss 0.43410614132881165\n",
      "train loss 0.3903191387653351\n",
      "train loss 0.4509756565093994\n",
      "train loss 0.3342556059360504\n",
      "train loss 0.48596158623695374\n",
      "train loss 0.49791547656059265\n",
      "train loss 0.32652732729911804\n",
      "train loss 0.43500658869743347\n",
      "train loss 0.3940524160861969\n",
      "train loss 0.5278993844985962\n",
      "train loss 0.4983246326446533\n",
      "train loss 0.40194690227508545\n",
      "train loss 0.43132978677749634\n",
      "train loss 0.41641074419021606\n",
      "train loss 0.49430370330810547\n",
      "train loss 0.4273948669433594\n",
      "train loss 0.4527128338813782\n",
      "train loss 0.40544790029525757\n",
      "train loss 0.32481199502944946\n",
      "train loss 0.37806734442710876\n",
      "train loss 0.2601918876171112\n",
      "train loss 0.49015727639198303\n",
      "train loss 0.31812840700149536\n",
      "train loss 0.4319522976875305\n",
      "train loss 0.3143818974494934\n",
      "train loss 0.38651514053344727\n",
      "train loss 0.32298171520233154\n",
      "train loss 0.3707069158554077\n",
      "train loss 0.4379999339580536\n",
      "train loss 0.4835631847381592\n",
      "train loss 0.4653124511241913\n",
      "train loss 0.3285149335861206\n",
      "train loss 0.2592723071575165\n",
      "train loss 0.3469492793083191\n",
      "train loss 0.5791109204292297\n",
      "train loss 0.4017174541950226\n",
      "train loss 0.41964688897132874\n",
      "train loss 0.5671554207801819\n",
      "train loss 0.5626680254936218\n",
      "train loss 0.3571610748767853\n",
      "train loss 0.3000667989253998\n",
      "train loss 0.35026803612709045\n",
      "train loss 0.43143555521965027\n",
      "train loss 0.43154364824295044\n",
      "train loss 0.34767740964889526\n",
      "train loss 0.4119294285774231\n",
      "train loss 0.4097483158111572\n",
      "train loss 0.403278112411499\n",
      "train loss 0.4218193292617798\n",
      "train loss 0.29738888144493103\n",
      "train loss 0.49478667974472046\n",
      "train loss 0.4120708107948303\n",
      "train loss 0.4355695843696594\n",
      "train loss 0.3818485140800476\n",
      "train loss 0.3414033055305481\n",
      "train loss 0.36900192499160767\n",
      "train loss 0.45549678802490234\n",
      "train loss 0.43241968750953674\n",
      "train loss 0.3822750747203827\n",
      "train loss 0.38559409976005554\n",
      "train loss 0.49992090463638306\n",
      "train loss 0.38392430543899536\n",
      "train loss 0.4465307593345642\n",
      "train loss 0.41859713196754456\n",
      "train loss 0.3930327594280243\n",
      "train loss 0.464041531085968\n",
      "train loss 0.34377092123031616\n",
      "train loss 0.4764000475406647\n",
      "train loss 0.2832791805267334\n",
      "train loss 0.40535521507263184\n",
      "train loss 0.34112608432769775\n",
      "train loss 0.40479978919029236\n",
      "train loss 0.4668009877204895\n",
      "train loss 0.23273074626922607\n",
      "train loss 0.3013194501399994\n",
      "train loss 0.32925882935523987\n",
      "train loss 0.48845401406288147\n",
      "train loss 0.29762136936187744\n",
      "train loss 0.44258376955986023\n",
      "train loss 0.471500039100647\n",
      "train loss 0.42115679383277893\n",
      "train loss 0.4781668186187744\n",
      "train loss 0.398415207862854\n",
      "train loss 0.5156555771827698\n",
      "train loss 0.3781147599220276\n",
      "train loss 0.320549339056015\n",
      "train loss 0.29318928718566895\n",
      "train loss 0.2950270473957062\n",
      "train loss 0.5088139176368713\n",
      "train loss 0.4493041932582855\n",
      "train loss 0.33939456939697266\n",
      "train loss 0.3499159812927246\n",
      "train loss 0.35467028617858887\n",
      "train loss 0.300773024559021\n",
      "train loss 0.3195992410182953\n",
      "train loss 0.3820599317550659\n",
      "train loss 0.244901642203331\n",
      "train loss 0.38937628269195557\n",
      "train loss 0.41411274671554565\n",
      "train loss 0.20283108949661255\n",
      "train loss 0.3975880742073059\n",
      "train loss 0.42499369382858276\n",
      "train loss 0.45184755325317383\n",
      "train loss 0.4554806649684906\n",
      "train loss 0.3403070271015167\n",
      "train loss 0.3580726087093353\n",
      "train loss 0.3117699921131134\n",
      "train loss 0.31834498047828674\n",
      "train loss 0.39893245697021484\n",
      "train loss 0.3648008108139038\n",
      "train loss 0.38738155364990234\n",
      "train loss 0.3943336606025696\n",
      "train loss 0.3849274516105652\n",
      "train loss 0.4538278579711914\n",
      "train loss 0.3986422121524811\n",
      "train loss 0.5311422348022461\n",
      "train loss 0.3390257954597473\n",
      "train loss 0.2711194157600403\n",
      "train loss 0.4175380766391754\n",
      "train loss 0.3431946933269501\n",
      "train loss 0.3197936713695526\n",
      "train loss 0.3933887481689453\n",
      "train loss 0.37358617782592773\n",
      "train loss 0.3499487638473511\n",
      "train loss 0.35645997524261475\n",
      "train loss 0.34291866421699524\n",
      "train loss 0.34746724367141724\n",
      "train loss 0.3985368013381958\n",
      "train loss 0.36939966678619385\n",
      "train loss 0.301726371049881\n",
      "train loss 0.4332675635814667\n",
      "train loss 0.35485684871673584\n",
      "train loss 0.35850590467453003\n",
      "train loss 0.40010640025138855\n",
      "train loss 0.3257678151130676\n",
      "train loss 0.3863433599472046\n",
      "train loss 0.3516175448894501\n",
      "train loss 0.4190811514854431\n",
      "train loss 0.3469836115837097\n",
      "train loss 0.4328833520412445\n",
      "train loss 0.3112139403820038\n",
      "train loss 0.45547404885292053\n",
      "train loss 0.3400648534297943\n",
      "train loss 0.4088713824748993\n",
      "train loss 0.3947518765926361\n",
      "train loss 0.43085700273513794\n",
      "train loss 0.28441837430000305\n",
      "train loss 0.31523069739341736\n",
      "train loss 0.3059689402580261\n",
      "train loss 0.5057244896888733\n",
      "train loss 0.44441643357276917\n",
      "train loss 0.3103105425834656\n",
      "train loss 0.31150346994400024\n",
      "train loss 0.4091721177101135\n",
      "train loss 0.3944863975048065\n",
      "train loss 0.2451360672712326\n",
      "train loss 0.2974335253238678\n",
      "train loss 0.36235788464546204\n",
      "train loss 0.3933176100254059\n",
      "train loss 0.4538578689098358\n",
      "train loss 0.30213025212287903\n",
      "train loss 0.4468247890472412\n",
      "train loss 0.3816584348678589\n",
      "train loss 0.38297200202941895\n",
      "train loss 0.2861868441104889\n",
      "train loss 0.2861348092556\n",
      "train loss 0.30210819840431213\n",
      "train loss 0.3675905168056488\n",
      "train loss 0.28087013959884644\n",
      "train loss 0.3741793632507324\n",
      "train loss 0.26843246817588806\n",
      "train loss 0.36238572001457214\n",
      "train loss 0.2949470579624176\n",
      "train loss 0.32921117544174194\n",
      "train loss 0.345319926738739\n",
      "train loss 0.3143341541290283\n",
      "train loss 0.26256710290908813\n",
      "train loss 0.29133665561676025\n",
      "train loss 0.2511855363845825\n",
      "train loss 0.4700794219970703\n",
      "train loss 0.26780974864959717\n",
      "train loss 0.3753490447998047\n",
      "train loss 0.28507551550865173\n",
      "train loss 0.37554192543029785\n",
      "train loss 0.40117454528808594\n",
      "train loss 0.2819942235946655\n",
      "train loss 0.2833172678947449\n",
      "train loss 0.3544729948043823\n",
      "train loss 0.45738911628723145\n",
      "train loss 0.3577866852283478\n",
      "train loss 0.3464439809322357\n",
      "train loss 0.48566436767578125\n",
      "train loss 0.31103935837745667\n",
      "train loss 0.24336646497249603\n",
      "train loss 0.3742094337940216\n",
      "train loss 0.44943660497665405\n",
      "train loss 0.27789703011512756\n",
      "train loss 0.27614742517471313\n",
      "train loss 0.36399996280670166\n",
      "train loss 0.3734775185585022\n",
      "train loss 0.22017957270145416\n",
      "train loss 0.3478791117668152\n",
      "train loss 0.32403573393821716\n",
      "train loss 0.42557403445243835\n",
      "train loss 0.30215972661972046\n",
      "train loss 0.18325723707675934\n",
      "train loss 0.32989904284477234\n",
      "train loss 0.24669243395328522\n",
      "train loss 0.3093683123588562\n",
      "train loss 0.2847268283367157\n",
      "train loss 0.2628873586654663\n",
      "train loss 0.3273237645626068\n",
      "train loss 0.34407463669776917\n",
      "train loss 0.33782631158828735\n",
      "train loss 0.3779808282852173\n",
      "train loss 0.35054290294647217\n",
      "train loss 0.299085408449173\n",
      "train loss 0.3321481943130493\n",
      "train loss 0.30281737446784973\n",
      "train loss 0.32840681076049805\n",
      "train loss 0.3962303400039673\n",
      "train loss 0.29794028401374817\n",
      "train loss 0.33605292439460754\n",
      "train loss 0.3106140196323395\n",
      "train loss 0.2725602686405182\n",
      "train loss 0.28634798526763916\n",
      "train loss 0.23408722877502441\n",
      "train loss 0.3858717978000641\n",
      "train loss 0.34209731221199036\n",
      "train loss 0.3563345670700073\n",
      "train loss 0.35728347301483154\n",
      "train loss 0.3165901005268097\n",
      "train loss 0.39188358187675476\n",
      "train loss 0.6159299612045288\n",
      "train loss 0.4910547137260437\n",
      "train loss 0.5407586693763733\n",
      "train loss 0.3473447859287262\n",
      "train loss 0.258261114358902\n",
      "train loss 0.21918177604675293\n",
      "train loss 0.2815176844596863\n",
      "train loss 0.3058406114578247\n",
      "train loss 0.25172826647758484\n",
      "train loss 0.38358405232429504\n",
      "train loss 0.3707747161388397\n",
      "train loss 0.45939135551452637\n",
      "train loss 0.32498645782470703\n",
      "train loss 0.3081679344177246\n",
      "train loss 0.3262959420681\n",
      "train loss 0.27601075172424316\n",
      "train loss 0.31047698855400085\n",
      "train loss 0.3133396804332733\n",
      "train loss 0.2935901880264282\n",
      "train loss 0.40195009112358093\n",
      "train loss 0.3190944194793701\n",
      "train loss 0.35272592306137085\n",
      "train loss 0.3211495578289032\n",
      "train loss 0.2803414762020111\n",
      "train loss 0.29860880970954895\n",
      "train loss 0.1996234506368637\n",
      "train loss 0.36636149883270264\n",
      "train loss 0.4730405807495117\n",
      "train loss 0.3191784918308258\n",
      "train loss 0.283066987991333\n",
      "train loss 0.2921859920024872\n",
      "train loss 0.2962598502635956\n",
      "train loss 0.39181584119796753\n",
      "train loss 0.3786226809024811\n",
      "train loss 0.28150418400764465\n",
      "train loss 0.4329586327075958\n",
      "train loss 0.2509128749370575\n",
      "train loss 0.4114258289337158\n",
      "train loss 0.3013875186443329\n",
      "train loss 0.3034484386444092\n",
      "train loss 0.41919583082199097\n",
      "train loss 0.49185967445373535\n",
      "train loss 0.40540698170661926\n",
      "train loss 0.26453110575675964\n",
      "train loss 0.33609166741371155\n",
      "train loss 0.2780512273311615\n",
      "train loss 0.3176373839378357\n",
      "train loss 0.33262309432029724\n",
      "train loss 0.32308077812194824\n",
      "train loss 0.2906721830368042\n",
      "train loss 0.3291139602661133\n",
      "train loss 0.3784765303134918\n",
      "train loss 0.3999518156051636\n",
      "train loss 0.21224184334278107\n",
      "train loss 0.26319852471351624\n",
      "train loss 0.27262163162231445\n",
      "train loss 0.24926653504371643\n",
      "train loss 0.32657158374786377\n",
      "train loss 0.35744693875312805\n",
      "train loss 0.22062034904956818\n",
      "train loss 0.34813088178634644\n",
      "train loss 0.3047003149986267\n",
      "train loss 0.3744753897190094\n",
      "train loss 0.3575625419616699\n",
      "train loss 0.2826600968837738\n",
      "train loss 0.38968411087989807\n",
      "train loss 0.3406270742416382\n",
      "train loss 0.4383525550365448\n",
      "train loss 0.2499234974384308\n",
      "train loss 0.2609885334968567\n",
      "train loss 0.24884963035583496\n",
      "train loss 0.34237080812454224\n",
      "train loss 0.2534753084182739\n",
      "train loss 0.48987531661987305\n",
      "train loss 0.27864933013916016\n",
      "train loss 0.3334055244922638\n",
      "train loss 0.4498651623725891\n",
      "train loss 0.5178791880607605\n",
      "train loss 0.2616097927093506\n",
      "train loss 0.2182525247335434\n",
      "train loss 0.2359786331653595\n",
      "train loss 0.24988336861133575\n",
      "train loss 0.31792372465133667\n",
      "train loss 0.34242963790893555\n",
      "train loss 0.3536510467529297\n",
      "train loss 0.3198808431625366\n",
      "train loss 0.40940359234809875\n",
      "train loss 0.36698803305625916\n",
      "train loss 0.3008304834365845\n",
      "train loss 0.3385534882545471\n",
      "train loss 0.44747471809387207\n",
      "train loss 0.37279000878334045\n",
      "train loss 0.31181570887565613\n",
      "train loss 0.23415783047676086\n",
      "train loss 0.3619548976421356\n",
      "train loss 0.25491440296173096\n",
      "train loss 0.36993271112442017\n",
      "train loss 0.32458531856536865\n",
      "train loss 0.31040704250335693\n",
      "train loss 0.429697722196579\n",
      "train loss 0.20555195212364197\n",
      "train loss 0.37468090653419495\n",
      "train loss 0.2788127660751343\n",
      "train loss 0.22877943515777588\n",
      "train loss 0.2741313576698303\n",
      "train loss 0.43131351470947266\n",
      "train loss 0.3292521834373474\n",
      "train loss 0.4912939667701721\n",
      "train loss 0.2676718533039093\n",
      "train loss 0.32891467213630676\n",
      "train loss 0.3329259157180786\n",
      "train loss 0.27664270997047424\n",
      "train loss 0.28853556513786316\n",
      "train loss 0.32398492097854614\n",
      "train loss 0.27631306648254395\n",
      "train loss 0.27951735258102417\n",
      "train loss 0.5229346752166748\n",
      "train loss 0.36207133531570435\n",
      "train loss 0.32961368560791016\n",
      "train loss 0.24310143291950226\n",
      "train loss 0.35049161314964294\n",
      "train loss 0.24758079648017883\n",
      "train loss 0.3101573884487152\n",
      "train loss 0.2945082485675812\n",
      "train loss 0.2732657492160797\n",
      "train loss 0.2801302373409271\n",
      "train loss 0.2112995684146881\n",
      "train loss 0.16752316057682037\n",
      "train loss 0.2721589207649231\n",
      "train loss 0.278900682926178\n",
      "train loss 0.2290402203798294\n",
      "train loss 0.2707153558731079\n",
      "train loss 0.3464789390563965\n",
      "train loss 0.2313777208328247\n",
      "train loss 0.3427388668060303\n",
      "train loss 0.4041869342327118\n",
      "train loss 0.2935839891433716\n",
      "train loss 0.2841629087924957\n",
      "train loss 0.29484254121780396\n",
      "train loss 0.29020676016807556\n",
      "train loss 0.33977705240249634\n",
      "train loss 0.2689041793346405\n",
      "train loss 0.31418272852897644\n",
      "train loss 0.3494211435317993\n",
      "train loss 0.41394633054733276\n",
      "train loss 0.36891451478004456\n",
      "train loss 0.26910772919654846\n",
      "train loss 0.3376624584197998\n",
      "train loss 0.37104469537734985\n",
      "train loss 0.25874021649360657\n",
      "train loss 0.2672974169254303\n",
      "train loss 0.4501781463623047\n",
      "train loss 0.34099385142326355\n",
      "train loss 0.3520439863204956\n",
      "train loss 0.3936530351638794\n",
      "train loss 0.3062659502029419\n",
      "train loss 0.2747695744037628\n",
      "train loss 0.30427005887031555\n",
      "train loss 0.3041044771671295\n",
      "train loss 0.23709720373153687\n",
      "train loss 0.3653944134712219\n",
      "train loss 0.1990092694759369\n",
      "train loss 0.13144856691360474\n",
      "train loss 0.260672926902771\n",
      "train loss 0.22210568189620972\n",
      "train loss 0.3127618134021759\n",
      "train loss 0.3541693687438965\n",
      "train loss 0.3459748923778534\n",
      "train loss 0.2667807638645172\n",
      "train loss 0.2615543603897095\n",
      "train loss 0.30778539180755615\n",
      "train loss 0.29088523983955383\n",
      "train loss 0.3022669553756714\n",
      "train loss 0.22435204684734344\n",
      "train loss 0.39300113916397095\n",
      "train loss 0.35149502754211426\n",
      "train loss 0.31114083528518677\n",
      "train loss 0.3492864668369293\n",
      "train loss 0.2134878933429718\n",
      "train loss 0.27924108505249023\n",
      "train loss 0.21475009620189667\n",
      "train loss 0.32282817363739014\n",
      "train loss 0.35281455516815186\n",
      "train loss 0.30813780426979065\n",
      "train loss 0.3015206754207611\n",
      "train loss 0.2523386478424072\n",
      "train loss 0.3706275522708893\n",
      "train loss 0.29599103331565857\n",
      "train loss 0.21657247841358185\n",
      "train loss 0.20265744626522064\n",
      "train loss 0.26436305046081543\n",
      "train loss 0.26581090688705444\n",
      "train loss 0.20447322726249695\n",
      "train loss 0.24538521468639374\n",
      "train loss 0.22858747839927673\n",
      "train loss 0.3028983771800995\n",
      "train loss 0.32156360149383545\n",
      "train loss 0.33357301354408264\n",
      "train loss 0.24743860960006714\n",
      "train loss 0.3553617000579834\n",
      "train loss 0.32814231514930725\n",
      "train loss 0.4014856219291687\n",
      "train loss 0.25129222869873047\n",
      "train loss 0.3571527898311615\n",
      "train loss 0.2992470860481262\n",
      "train loss 0.14757497608661652\n",
      "train loss 0.26735615730285645\n",
      "train loss 0.3346135914325714\n",
      "train loss 0.3033022880554199\n",
      "train loss 0.4286796450614929\n",
      "train loss 0.24372152984142303\n",
      "train loss 0.31783485412597656\n",
      "train loss 0.29274147748947144\n",
      "train loss 0.25884538888931274\n",
      "train loss 0.27910757064819336\n",
      "train loss 0.19038887321949005\n",
      "train loss 0.28565534949302673\n",
      "train loss 0.3288026750087738\n",
      "train loss 0.34423667192459106\n",
      "train loss 0.3755733370780945\n",
      "train loss 0.2806743383407593\n",
      "train loss 0.3392542004585266\n",
      "train loss 0.3569558262825012\n",
      "train loss 0.19375154376029968\n",
      "train loss 0.33290040493011475\n",
      "train loss 0.28148162364959717\n",
      "train loss 0.29794052243232727\n",
      "train loss 0.31538739800453186\n",
      "train loss 0.2941127419471741\n",
      "train loss 0.2957517206668854\n",
      "train loss 0.27133628726005554\n",
      "train loss 0.25736117362976074\n"
     ]
    }
   ],
   "source": [
    "def train_loop(data_loader, model, loss_fn, optimizer):\n",
    "    N = len(data_loader.dataset)\n",
    "    for batch, (x, y) in enumerate(data_loader):\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch == 0:\n",
    "            print(f\"train loss {format(loss.item())}\")\n",
    "    return error\n",
    "\n",
    "def test_loop(data_loader, model, loss_fn):\n",
    "    N = len(data_loader.dataset)\n",
    "    batch_size = len(data_loader)\n",
    "    loss, correct = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            pred = model(x)\n",
    "            loss += loss_fn(pred, y).item()\n",
    "            correct += torch.sum((pred > .5) == y)\n",
    "    loss = loss / batch_size\n",
    "    correct = correct/N\n",
    "    # print(f\"test loss: {loss}\")\n",
    "    return loss, correct\n",
    "\n",
    "loss = 0\n",
    "for i in range(epochs):\n",
    "    train_loop(train_data_loader, model, loss_fn, optimizer)\n",
    "    l, _ = test_loop(val_data_loader, model, loss_fn)\n",
    "    loss += l\n",
    "\n",
    "train_loss, train_correct = test_loop(train_data_loader, model, loss_fn)\n",
    "test_loss, test_correct = test_loop(test_data_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train results: 0.28555152216289614, correct: 0.8787999749183655\n",
      "avg val loss: 0.45002343207323\n",
      "test results: 0.5378207728266716, correct: 0.7972000241279602\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# np.asarray([model(test_data_loader.dataset[:][0]).T > 0])\n",
    "\n",
    "error = np.asarray(error).flatten()\n",
    "plt.plot(error)\n",
    "print(f\"train results: {train_loss}, correct: {train_correct}\")\n",
    "print(f\"avg val loss: {loss/epochs}\")\n",
    "print(f\"test results: {test_loss}, correct: {test_correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b4957a3169536379e8f75f7e9878614f7937738a9df979092ace7c23c9fbae51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
